{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Spam Classification with Naive Bayes\n",
    "\n",
    "\n",
    "***Context***\n",
    "The SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according being ham (legitimate) or spam. Your job is to build a classifier to identify message as ham or spam. \n",
    "\n",
    "***Content***\n",
    "The files contain one message per line. Each line is composed by two columns: first contains the label (ham or spam) and second coloumn contains the raw text.The dataset has been taken from [Kaggle](https://www.kaggle.com/uciml/sms-spam-collection-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "source": [
    "**Checking the Length of SMS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "4bb2b45f846f75fae48ea7e06f9abe43e3518f6e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                sms\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "df_sms = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "107c0fe13d78fab5a4f9602e3f430f24c74ad158"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will Ã_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                                sms\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham             Will Ã_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sms.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "9ce3b9b6d760ac0c081cd1673c38f55c09827adb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of observations in each label spam and ham\n",
    "df_sms.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "2f020baef301472ba7280ff152916cf36360a38d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5572</td>\n",
       "      <td>5572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2</td>\n",
       "      <td>5169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>ham</td>\n",
       "      <td>Sorry, I'll call later</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>4825</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                     sms\n",
       "count   5572                    5572\n",
       "unique     2                    5169\n",
       "top      ham  Sorry, I'll call later\n",
       "freq    4825                      30"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sms.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "0e64fe53089b02124da0f41296a7059ad499ce16"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                                sms  length\n",
       "0   ham  Go until jurong point, crazy.. Available only ...     111\n",
       "1   ham                      Ok lar... Joking wif u oni...      29\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3   ham  U dun say so early hor... U c already then say...      49\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sms['length'] = df_sms['sms'].apply(len)\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum length of SMS is 910\n"
     ]
    }
   ],
   "source": [
    "print (f\"Maximum length of SMS is {max(df_sms['length'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimim length of SMS is 2\n"
     ]
    }
   ],
   "source": [
    "print (f\"Minimim length of SMS is {min(df_sms['length'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Frequency')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = list(df_sms['length'])\n",
    "binsize = 30\n",
    "plt.hist(data, bins=range(min(data), max(data) + binsize, binsize))\n",
    "\n",
    "plt.title('Histogram of SMS length')\n",
    "plt.xlabel(\"Bins\")\n",
    "plt.ylabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "1c2a9fc260cef3996ca2515e1946e7413c685bd3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x1a180d20f0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x1a18e3d390>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1gAAAEQCAYAAACtC9WAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHw5JREFUeJzt3X+wX3V95/HnS0DaouVnYDEJhmq06rZEeotM2W5RLD93NuiUFraVyNCNM4VZnXW2jW5n0LZ0Y2cVy7S6RkFiq8WotaQlVVNq69gWIdAYgahEjBASkig/xKVlJbz3j++55Zvkm+Qm99zv+d57n4+ZO9/z/ZzP+d73PXOSc17fc87npKqQJEmSJE3e87ouQJIkSZJmCgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIDliRJkiS1xIAlTUCSzUle33UdkiRJGm0GLEmSJElqiQFLkiRJklpiwJImblGSDUmeSPLJJD+S5Ngkf5VkZ5LHmul54wsk+bskv5fkH5P8IMlfJjk+yceTfD/JnUkWdPcnSZJ08JL8VpKHkzyZ5BtJzknyriSfbvaRTya5O8lpfcssS/KtZt59Sd7QN+/NSf4hyXVJHk/yQJKfa9ofSrIjyZJu/lrp4BiwpIn7ZeB84FTgp4E30/s39FHgxcApwL8Af7THcpcCbwLmAi8B/qlZ5jhgI3DN1JcuSVI7krwcuBr42ap6IXAesLmZvRj4FL193CeAv0hyRDPvW8DPA0cD7wb+NMnJfR/9GmADcHyz7M3AzwIvBX4N+KMkL5i6v0xqhwFLmrjrq2prVT0K/CWwqKq+V1WfqaqnqupJ4FrgF/ZY7qNV9a2qegL4a+BbVfU3VfUMvZ3Qq4f6V0iSNDm7gCOBVyY5oqo2V9W3mnl3VdWnq+qHwPuAHwHOBKiqTzX70Wer6pPA/cAZfZ/77ar6aFXtAj4JzAd+p6qerqovAP+PXtiSRpoBS5q4R/qmnwJekOTHknwoyXeSfB/4EnBMksP6+m7vm/6XAe/9Nk6SNG1U1SbgbcC7gB1Jbk7yomb2Q339ngW2AC8CSHJ5kvXNJYCPA/8eOKHvo/fcP1JV7jM17RiwpMl5O/By4DVV9ePAf2za011JkiRNrar6RFX9B3qXyBfwnmbW/PE+SZ4HzAO2Jnkx8GF6lxYeX1XHAPfg/lIzkAFLmpwX0vtG7fEkx+H9VJKkGS7Jy5O8LsmRwL/S2w/uamb/TJI3Jjmc3lmup4HbgaPoBbGdzWdcQe8MljTjGLCkyXk/8KPAd+ntQD7XbTmSJE25I4Hl9PZ9jwAnAu9s5t0C/ArwGL0Bnt5YVT+sqvuA99Ib6Gk78FPAPwy5bmkoUlVd1yBJkqRpLsm7gJdW1a91XYvUJc9gSZIkSVJLDFiSJEmS1BIvEZQkSZKklngGS5IkSZJaYsCSJEmSpJYc3nUB+3PCCSfUggULui5DkmaVu+6667tVNafrOrRv7h8lafgmun8c6YC1YMEC1q1b13UZkjSrJPlO1zVo/9w/StLwTXT/6CWCkiRJktQSA5YkSZIktcSAJUmSJEktMWBJktSiJD+S5I4kX01yb5J3N+2nJvlKkvuTfDLJ85v2I5v3m5r5C7qsX5I0OQYsSZLa9TTwuqo6DVgEnJ/kTOA9wHVVtRB4DLiy6X8l8FhVvRS4ruknSZqmDFiSJLWoen7QvD2i+SngdcCnm/aVwMXN9OLmPc38c5JkSOVKklpmwJIkqWVJDkuyHtgBrAW+BTxeVc80XbYAc5vpucBDAM38J4Djh1uxJKktBixJklpWVbuqahEwDzgDeMWgbs3roLNVtWdDkqVJ1iVZt3PnzvaKlSS1aqQfNDxZC5bdOqnlNy+/qKVKJEmzUVU9nuTvgDOBY5Ic3pylmgdsbbptAeYDW5IcDhwNPDrgs1YAKwDGxsb2CmCSpp7HlpoIz2BJktSiJHOSHNNM/yjwemAj8EXgl5puS4BbmunVzXua+X9bVQYoSZqmZvQZLEmSOnAysDLJYfS+yFxVVX+V5D7g5iS/B/wzcEPT/wbgT5Jsonfm6tIuipYktcOAJUlSi6pqA/DqAe0P0Lsfa8/2fwUuGUJpkqQh8BJBSZIkSWqJAUuSJEmSWmLAkiRJkqSWGLAkSZIkqSUGLEmSJElqiQFLkiRJklpiwJIkSZKklhiwJEmSJKklBixJkiRJaskBA1aS+Um+mGRjknuTvLVpPy7J2iT3N6/HNu1Jcn2STUk2JDm977OWNP3vT7Jk6v4sSZIkSRq+iZzBegZ4e1W9AjgTuCrJK4FlwG1VtRC4rXkPcAGwsPlZCnwQeoEMuAZ4DXAGcM14KJMkSZKkmeCAAauqtlXV3c30k8BGYC6wGFjZdFsJXNxMLwY+Vj23A8ckORk4D1hbVY9W1WPAWuD8Vv8aSZIkSerQQd2DlWQB8GrgK8BJVbUNeiEMOLHpNhd4qG+xLU3bvtr3/B1Lk6xLsm7nzp0HU54kSZIkdWrCASvJC4DPAG+rqu/vr+uAttpP++4NVSuqaqyqxubMmTPR8iRJkiSpcxMKWEmOoBeuPl5Vf940b28u/aN53dG0bwHm9y0+D9i6n3ZJkiRJmhEmMopggBuAjVX1vr5Zq4HxkQCXALf0tV/ejCZ4JvBEcwnh54FzkxzbDG5xbtMmSZIkSTPC4RPocxbwJuBrSdY3be8ElgOrklwJPAhc0sxbA1wIbAKeAq4AqKpHk/wucGfT73eq6tFW/gpJkiRJGgEHDFhV9WUG3z8FcM6A/gVctY/PuhG48WAKlCRJkqTp4qBGEZQkSZIk7ZsBS5IkSZJaYsCSJEmSpJYYsCRJkiSpJQYsSZIkSWqJAUuSJEmSWmLAkiRJkqSWGLAkSZIkqSUGLEmSWpRkfpIvJtmY5N4kb23a35Xk4STrm58L+5Z5R5JNSb6R5LzuqpckTdbhXRcgSdIM8wzw9qq6O8kLgbuSrG3mXVdV/7u/c5JXApcCrwJeBPxNkpdV1a6hVi1JaoVnsCRJalFVbauqu5vpJ4GNwNz9LLIYuLmqnq6qbwObgDOmvlJJ0lQwYEmSNEWSLABeDXylabo6yYYkNyY5tmmbCzzUt9gWBgSyJEuTrEuybufOnVNYtSRpMgxYkiRNgSQvAD4DvK2qvg98EHgJsAjYBrx3vOuAxWuvhqoVVTVWVWNz5syZoqolSZNlwJIkqWVJjqAXrj5eVX8OUFXbq2pXVT0LfJjnLgPcAszvW3wesHWY9UqS2mPAkiSpRUkC3ABsrKr39bWf3NftDcA9zfRq4NIkRyY5FVgI3DGseiVJ7XIUQUmS2nUW8Cbga0nWN23vBC5Lsoje5X+bgbcAVNW9SVYB99EbgfAqRxCUpOnLgCVJUouq6ssMvq9qzX6WuRa4dsqKkiQNjZcISpIkSVJLDFiSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUmSJEktMWBJkiRJUksMWJIkSZLUEgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUmSJEktMWBJkiRJUksOGLCS3JhkR5J7+treleThJOubnwv75r0jyaYk30hyXl/7+U3bpiTL2v9TJEmSJKlbEzmDdRNw/oD266pqUfOzBiDJK4FLgVc1y3wgyWFJDgP+GLgAeCVwWdNXkiRJkmaMww/Uoaq+lGTBBD9vMXBzVT0NfDvJJuCMZt6mqnoAIMnNTd/7DrpiSZIkSRpRk7kH6+okG5pLCI9t2uYCD/X12dK07atdkiRJkmaMQw1YHwReAiwCtgHvbdozoG/tp30vSZYmWZdk3c6dOw+xPEmSupFkfpIvJtmY5N4kb23aj0uyNsn9zeuxTXuSXN/co7whyend/gWSpMk4pIBVVduraldVPQt8mOcuA9wCzO/rOg/Yup/2QZ+9oqrGqmpszpw5h1KeJEldegZ4e1W9AjgTuKq573gZcFtVLQRua95D7/7khc3PUnpfYkqSpqlDClhJTu57+wZgfITB1cClSY5Mciq9ncUdwJ3AwiSnJnk+vYEwVh962ZIkjaaq2lZVdzfTTwIb6V0WvxhY2XRbCVzcTC8GPlY9twPH7LGflSRNIwcc5CLJnwFnAyck2QJcA5ydZBG9y/w2A28BqKp7k6yiN3jFM8BVVbWr+Zyrgc8DhwE3VtW9rf81kiSNkGaQqFcDXwFOqqpt0AthSU5suu3rPuVtw6tUktSWiYwieNmA5hv20/9a4NoB7WuANQdVXccWLLt1UstvXn5RS5VIkqabJC8APgO8raq+nwy6HbnXdUDbXvcpJ1lK7xJCTjnllLbKlCS1bDKjCEqSpAGSHEEvXH28qv68ad4+fulf87qjaZ/QfcreoyxJ04MBS5KkFqV3quoGYGNVva9v1mpgSTO9BLilr/3yZjTBM4Enxi8llCRNPwe8RFCSJB2Us4A3AV9Lsr5peyewHFiV5ErgQeCSZt4a4EJgE/AUcMVwy5UktcmAJUlSi6rqywy+rwrgnAH9C7hqSouSJA2NlwhKkiRJUksMWJIkSZLUEgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUmSJEktMWBJkiRJUksMWJIkSZLUEgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUmSJEktMWBJkiRJUksMWJIkSZLUEgOWJEktSnJjkh1J7ulre1eSh5Osb34u7Jv3jiSbknwjyXndVC1JaosBS5Kkdt0EnD+g/bqqWtT8rAFI8krgUuBVzTIfSHLY0CqVJLXOgCVJUouq6kvAoxPsvhi4uaqerqpvA5uAM6asOEnSlDNgSZI0HFcn2dBcQnhs0zYXeKivz5ambS9JliZZl2Tdzp07p7pWSdIhMmBJkjT1Pgi8BFgEbAPe27RnQN8a9AFVtaKqxqpqbM6cOVNTpSRp0gxYkiRNsaraXlW7qupZ4MM8dxngFmB+X9d5wNZh1ydJao8BS5KkKZbk5L63bwDGRxhcDVya5MgkpwILgTuGXZ8kqT2Hd12AJEkzSZI/A84GTkiyBbgGODvJInqX/20G3gJQVfcmWQXcBzwDXFVVu7qoW5LUDgOWJEktqqrLBjTfsJ/+1wLXTl1FkqRhOuAlgvt4YOJxSdYmub95PbZpT5Lrmwcmbkhyet8yS5r+9ydZMjV/jiRJkiR1ZyL3YN3E3g9MXAbcVlULgdua9wAX0Lt+fCGwlN6oSSQ5jt4lEq+hd2PvNX1D1EqSJEnSjHDAgLWPByYuBlY20yuBi/vaP1Y9twPHNDf2ngesrapHq+oxYC2Dn3IvSZIkSdPWoY4ieFJVbQNoXk9s2vf1wEQfpChJkiRpxmt7mPZ9PTDRBylKkiRJmvEONWBtH3+mR/O6o2nf1wMTfZCiJEmSpBnvUIdpXw0sAZY3r7f0tV+d5GZ6A1o8UVXbknwe+P2+gS3OBd5x6GVLkiRJB2fBslu7LkGzwAED1j4emLgcWJXkSuBB4JKm+xrgQmAT8BRwBUBVPZrkd4E7m36/U1V7DpwhSZIkSdPaAQPWPh6YCHDOgL4FXLWPz7kRuPGgqpMkSZKkaaTtQS4kSZIkadYyYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUmSJEktMWBJkiRJUksMWJIkSZLUEgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSwxYkiRJktQSA5YkSZIktcSAJUlSi5LcmGRHknv62o5LsjbJ/c3rsU17klyfZFOSDUlO765ySVIbDFiSJLXrJuD8PdqWAbdV1ULgtuY9wAXAwuZnKfDBIdUoSZoiBixJklpUVV8CHt2jeTGwspleCVzc1/6x6rkdOCbJycOpVJI0FQxYkiRNvZOqahtA83pi0z4XeKiv35amTZI0TRmwJEnqTga01cCOydIk65Ks27lz5xSXJUk6VAYsSZKm3vbxS/+a1x1N+xZgfl+/ecDWQR9QVSuqaqyqxubMmTOlxUqSDp0BS5KkqbcaWNJMLwFu6Wu/vBlN8EzgifFLCSVJ09PhXRcgSdJMkuTPgLOBE5JsAa4BlgOrklwJPAhc0nRfA1wIbAKeAq4YesGSpFYZsCRJalFVXbaPWecM6FvAVVNbkSRpmLxEUJIkSZJaYsCSJEmSpJYYsCRJkiSpJQYsSZIkSWqJAUuSJEmSWuIoglNowbJbJ7X85uUXtVSJJEmSpGHwDJYkSZIktcSAJUmSJEktMWBJkiRJUku8B0uSJEnTwmTvb5eGwTNYkiRJktQSA5YkSZIktcSAJUmSJEktmVTASrI5ydeSrE+yrmk7LsnaJPc3r8c27UlyfZJNSTYkOb2NP0CSJEmSRkUbZ7BeW1WLqmqseb8MuK2qFgK3Ne8BLgAWNj9LgQ+28LslSZIkaWRMxSWCi4GVzfRK4OK+9o9Vz+3AMUlOnoLfL0mSJEmdmGzAKuALSe5KsrRpO6mqtgE0ryc27XOBh/qW3dK07SbJ0iTrkqzbuXPnJMuTJEmSpOGZ7HOwzqqqrUlOBNYm+fp++mZAW+3VULUCWAEwNja213xJkiRJGlWTOoNVVVub1x3AZ4EzgO3jl/41rzua7luA+X2LzwO2Tub3S5IkSdIoOeSAleSoJC8cnwbOBe4BVgNLmm5LgFua6dXA5c1ogmcCT4xfSihJkiRJM8FkLhE8CfhskvHP+URVfS7JncCqJFcCDwKXNP3XABcCm4CngCsm8bslSZIkaeQccsCqqgeA0wa0fw84Z0B7AVcd6u+TJEmSpFE32UEuNIUWLLt1UstvXn5RS5VIkiRJmoipeA6WJEmSJM1KBixJkiRJaokBS5IkSZJa4j1YkiQNSZLNwJPALuCZqhpLchzwSWABsBn45ap6rKsaJUmT4xksSZKG67VVtaiqxpr3y4DbqmohcFvzXpI0TRmwJEnq1mJgZTO9Eri4w1okSZPkJYKSJA1PAV9IUsCHqmoFcFJVbQOoqm1JThy0YJKlwFKAU045ZVj1SmqRj+CZHQxYkiQNz1lVtbUJUWuTfH2iCzZhbAXA2NhYTVWBkqTJMWBJkjQkVbW1ed2R5LPAGcD2JCc3Z69OBnZ0WqQ0xSZ7Fkcadd6DJUnSECQ5KskLx6eBc4F7gNXAkqbbEuCWbiqUJLXBM1iSJA3HScBnk0Bv//uJqvpckjuBVUmuBB4ELumwRknSJBmwJEkagqp6ADhtQPv3gHOGX5EkaSp4iaAkSZIktcSAJUmSJEktMWBJkiRJUksMWJIkSZLUEgOWJEmSJLXEgCVJkiRJLTFgSZIkSVJLDFiSJEmS1BIfNCxJkqQJW7Ds1q5LkEaaZ7AkSZIkqSUGLEmSJElqiQFLkiRJklriPViSJEmziPdQSVPLM1iSJEmS1BIDliRJkiS1xIAlSZIkSS0xYEmSJElSSxzkQpIk6SBMdpCIzcsvaqkSSaPIM1iSJEmS1BIDliRJkiS1xEsEJUmSphGfYyWNNgOWJEmaVqb7PVAGJGlmM2BJkiRJ08B0/3JhtvAeLEmSJElqiWewZjC/5ZAkSZKGa+gBK8n5wB8ChwEfqarlw65BkqRRM532j9P9HqLpXr+k0TbUgJXkMOCPgV8EtgB3JlldVfcNsw5NTNc7IM+gSZot3D9K0swx7DNYZwCbquoBgCQ3A4sBdyCSpNlsqPvHrr9Ak6SZbNgBay7wUN/7LcBrhlyDponpfgDgGThJB8H9o6SR1/X9/V3//okadsDKgLbarUOyFFjavP1Bkm9M4vedAHx3EsvPVK6XwVpdL3lPW5/UObeXfZup6+bFXRcwCw17/zjTzdR/m21zPU3cjFhXQzo22ee66vrYqIXfP6H947AD1hZgft/7ecDW/g5VtQJY0cYvS7Kuqsba+KyZxPUymOtlMNfLvrlu1KKh7h9nOv9tTozraeJcVxPnuhr+c7DuBBYmOTXJ84FLgdVDrkGSpFHj/lGSZoihnsGqqmeSXA18nt4wtDdW1b3DrEGSpFHj/lGSZo6hPwerqtYAa4b067yUYjDXy2Cul8FcL/vmulFrhrx/nOn8tzkxrqeJc11N3KxfV6mqA/eSJEmSJB3QsO/BkiRJkqQZy4AlSZIkSS0Z+j1YUyXJT9J76v1ces8O2QqsrqqNnRYmSZIkadaYEfdgJfkt4DLgZnrPEoHeM0QuBW6uquVd1TYqkpxEX/isqu0dlzQykhwHVFU91nUto8LtZd/cXiRJ2p3HDbubKQHrm8CrquqHe7Q/H7i3qhZ2U1n3kiwC/g9wNPBw0zwPeBz4jaq6u6vaupTkFOAPgHPorYsAPw78LbCsqjZ3V1133F4Gc3uRRlOSo4F3ABcDc5rmHcAtwPKqeryr2kaVB8ITkyTAGex+ZdQdNRMOnFvkccNgM+USwWeBFwHf2aP95GbebHYT8Jaq+kp/Y5IzgY8Cp3VR1Aj4JPB+4FerahdAksOAS+idCT2zw9q6dBNuL4O4vUijaRW9LzrOrqpHAJL8O2AJ8CngFzusbaTs60A4yaw+EB4kybnAB4D72T00vDTJb1TVFzorbvTchMcNe5kpZ7DOB/6I3j+Eh5rmU4CXAldX1ee6qq1rSe7f1xm8JJuq6qXDrmkUHGC97HPeTOf2MpjbizSaknyjql5+sPNmoyTr2feB8IeqalYeCA+SZCNwwZ5XJyQ5FVhTVa/opLAR5HHDYDPiDFZVfS7Jy3juVG7o3Yt15/i3zbPYXye5FfgYz4XP+cDlwKwNnsBdST4ArGT39bIE+OfOquqe28tgbi/SaPpOkt8EVo5f6tZcAvdmnvu3qp6j9gxXAFV1e5KjuihohB3Oc/f093sYOGLItYw6jxsGmBFnsLR/SS7guREWx8Pn6qpa02lhHWruz7uSAesFuKGqnu6wvE65vezN7UUaTUmOBZbR+7d5Er17ZbbT+7f5nqp6tMPyRkqS64GXMPhA+NtVdXVXtY2aJO8AfpneJeD96+pSYFVV/a+uahtFHjfszYAlSZJmhCQ/T+9qlq95n8zePBCeuCSvYPC6uq/TwjQtGLBmuL4RlhYDJzbNs36EpSSH0zsjcTG7jxB0C70zEj/cz+IzltvLYG4v0mhKckdVndFM/zpwFfAXwLnAX/qYFmlqedww2PO6LkBTbhXwGPDaqjq+qo4HXktv+MxPdVpZt/4EWAS8G7gQuKiZPg340w7r6prby2BuL9Jo6r8f5i3AuVX1bnoB61e7KWk0JTk6yfIkG5N8r/nZ2LQd03V9o6QZPG18+ugkH0myIcknmnv89ByPGwbwDNYM5whLgx1gvXyzql427JpGgdvLYG4v0mhK8lXgbHpfGH++qsb65v1zVb26q9pGTZLP0xvSfuUeQ9q/GTinqhzSvpHk7qo6vZn+CPAI8GHgjcAvVNXFXdY3SjxuGMwzWDPfd5L8Zv83LklOSvJbzO4Rlh5LckmSf/s3kOR5SX6F3jcxs5Xby2BuL9JoOhq4C1gHHNcEBpK8gN59M3rOgqp6z3i4AqiqR5rLKE/psK5RN1ZVv11V36mq64AFXRc0YjxuGMCANfP9CnA88PdJHkvyKPB3wHH0RsiZrS4FfgnYnuSbSe6n9w3VG5t5s5Xby2Dj28sjzfbyTdxepM5V1YKq+omqOrV5HQ8PzwJv6LK2EeSB8MSdmOS/J3k78ONJ+sO6x86787hhAC8RnAWS/CS9J5DfXlU/6Gs/fzY/hHlckuPpfdP5/qr6ta7r6VKS1wBfr6onkvwYveGPTwfuBX6/qp7otMCONMO0X0ZvYIu7gQuAn6O3XlY4yIWkUbfHkPbjgxGMD2m/vKo8G99Ics0eTR+oqp3NGdI/qKrLu6hrVHmcuTcD1gyX5L/RG1VpI72b9N9aVbc08/7tGuPZJsnqAc2vo3d9OlX1n4db0WhIci9wWlU9k2QF8H+BzwDnNO1v7LTAjiT5OL0HT/4o8ARwFPBZeuslVbWkw/IkaVKSXFFVH+26junAdbU7jzMHO7zrAjTl/ivwM1X1gyQLgE8nWVBVf8jsvj59HnAf8BF6Q24H+FngvV0WNQKeV1XPNNNjff8xfjnJ+q6KGgE/VVU/3QzX/jDwoqraleRPga92XJskTda7AUPDxLiududx5gAGrJnvsPHTtVW1OcnZ9Db+FzOLN3xgDHgr8D+B/1FV65P8S1X9fcd1de2evm/nvppkrKrWJXkZMJsvg3tec5ngUcCP0bux/lHgSHYfJlqSRlKSDfuaBTj0eB/X1UHxOHMAA9bM90iSRVW1HqD5huE/ATcCP9Vtad2pqmeB65J8qnndjv8eAH4d+MMkvw18F/inJA/RuwH61zutrFs3AF8HDqMXyj+V5AHgTODmLguTpAk6CTiPvUc+DfCPwy9npLmuJs7jzAG8B2uGSzIPeKZ/WNa+eWdV1T90UNbISXIRcFZVvbPrWkZBkhcCP0EvdG6pqu0dl9S5JC8CqKqtzUM5Xw88WFV3dFuZJB1YkhuAj1bVlwfM+0RV/ZcOyhpJrquJ8zhzMAOWJEmSJLXEsfwlSZIkqSUGLEmSJElqiQFLkiRJklpiwJIkSZKklhiwJEmSJKkl/x/0Juk8T6DlVwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1008x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sms.hist(column='length', by='label', bins=20,figsize=(14,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "b187b7705bd949f2ae80f825657adb43b3bb57da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sms</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                sms  length\n",
       "0      0  Go until jurong point, crazy.. Available only ...     111\n",
       "1      0                      Ok lar... Joking wif u oni...      29\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...     155\n",
       "3      0  U dun say so early hor... U c already then say...      49\n",
       "4      0  Nah I don't think he goes to usf, he lives aro...      61"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#replace ham with 0 and spam with 1\n",
    "df_sms.loc[:,'label'] = df_sms.label.map({'ham':0, 'spam':1})\n",
    "print(df_sms.shape)\n",
    "df_sms.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "233831c7ecdeb8311cc2c5eaf1fb92098a8b4081"
   },
   "source": [
    "## Bag of Words Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "38f6a4ac426e4aae919a88080b3e06c691f95b2b"
   },
   "source": [
    "\n",
    "What we have here in our data set is a collection of text data (5,572 rows of data). Most ML algorithms rely on numerical data to be fed into them as input, and email/sms messages are usually text heavy.\n",
    "We need a way to represent text data for machine learning algorithm and the bag-of-words model helps us to achieve that task.\n",
    "It is a way of extracting features from the text for use in machine learning algorithms.\n",
    "In this approach, we use the tokenized words for each observation and find out the frequency of each token.\n",
    "Using a process which we will go through now, we can convert a collection of documents to a matrix, with each document being a row and each word(token) being the column, and the corresponding (row,column) values being the frequency of occurrence of each word or token in that document.\n",
    "\n",
    "For example:\n",
    "\n",
    "Lets say we have 4 documents as follows:\n",
    "\n",
    "**['Hello, how are you!',\n",
    "'Win money, win from home.',\n",
    "'Call me now',\n",
    "'Hello, Call you tomorrow?']**\n",
    "\n",
    "Our objective here is to convert this set of text to a frequency distribution matrix, as follows:\n",
    "<img src=\"https://image.ibb.co/casG7U/countvectorizer.png\" alt=\"table\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e7d98fe16bb796f82419fa591605f457b0d52836"
   },
   "source": [
    "Here as we can see, the documents are numbered in the rows, and each word is a column name, with the corresponding value being the frequency of that word in the document.\n",
    "\n",
    "Lets break this down and see how we can do this conversion using a small set of documents.\n",
    "\n",
    "To handle this, we will be using sklearns count vectorizer method which does the following:\n",
    "\n",
    "1.  It tokenizes the string(separates the string into individual words) and gives an integer ID to each token.\n",
    "2. It counts the occurrence of each of those tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b1160ede91a8705d6f2aa30db6a198493f29661"
   },
   "source": [
    "**Implementation of Bag of Words Approach in Python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c801dcc3d64d88c1336de3b9aad1a3a802f711f1"
   },
   "source": [
    "Step 1: Convert all strings to their lower case form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "74535171bb8c56beef46d910fc322c8c6f8879ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
     ]
    }
   ],
   "source": [
    "documents = ['Hello, how are you!',\n",
    "             'Win money, win from home.',\n",
    "             'Call me now.',\n",
    "             'Hello, Call hello you tomorrow?']\n",
    "\n",
    "lower_case_documents = []\n",
    "lower_case_documents = [d.lower() for d in documents]\n",
    "print(lower_case_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0c7114eeac1932122a01943d9fcdd52f13e86e80"
   },
   "source": [
    "Step 2: Removing all punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "0385f268056be9d84613fbc3a492d8b4695480bf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how are you',\n",
       " 'win money win from home',\n",
       " 'call me now',\n",
       " 'hello call hello you tomorrow']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sans_punctuation_documents = []\n",
    "import string\n",
    "\n",
    "for i in lower_case_documents:\n",
    "    sans_punctuation_documents.append(i.translate(str.maketrans(\"\",\"\", string.punctuation)))\n",
    "    \n",
    "sans_punctuation_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1a709cf9e09aa2ea1e67009ad19aaba8475fa6f"
   },
   "source": [
    "Step 3: Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "ef412ffa525e15273eaace2aae4e374a8933091c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['hello', 'how', 'are', 'you'],\n",
       " ['win', 'money', 'win', 'from', 'home'],\n",
       " ['call', 'me', 'now'],\n",
       " ['hello', 'call', 'hello', 'you', 'tomorrow']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]\n",
    "preprocessed_documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "95630beff35cb61b5fd1744b91ca37e7f6dd1d00"
   },
   "source": [
    "Step 4: Count frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "bdcd4677b6634fbe7f75c47262903efdb38100d1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n",
      " Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n",
      " Counter({'call': 1, 'me': 1, 'now': 1}),\n",
      " Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
     ]
    }
   ],
   "source": [
    "frequency_list = []\n",
    "import pprint\n",
    "from collections import Counter\n",
    "\n",
    "frequency_list = [Counter(d) for d in preprocessed_documents]\n",
    "pprint.pprint(frequency_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cc97056c261ef100eb715301ae09a068530791d9"
   },
   "source": [
    "**Implementing Bag of Words in scikit-learn**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "502cd98468dac23ba325d4e75fd997f730ce22a0"
   },
   "source": [
    "'''\n",
    "Here we will look to create a frequency matrix on a smaller document set to make sure we understand how the \n",
    "document-term matrix generation happens. We have created a sample document set 'documents'.\n",
    "'''\n",
    "documents = ['Hello, how are you!',\n",
    "                'Win money, win from home.',\n",
    "                'Call me now.',\n",
    "                'Hello, Call hello you tomorrow?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "7ca6a8661c2d14ce311a263c8f5b599f099bcd92"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea63b5f315ff995b52de58a8805bbe10af9c17d6"
   },
   "source": [
    "**Data preprocessing with  ()**\n",
    "\n",
    "In above step, we implemented a version of the CountVectorizer() method from scratch that entailed cleaning our data first. This cleaning involved converting all of our data to lower case and removing all punctuation marks. CountVectorizer() has certain parameters which take care of these steps for us. They are:\n",
    "\n",
    "lowercase = True\n",
    "\n",
    "The lowercase parameter has a default value of True which converts all of our text to its lower case form.\n",
    "\n",
    "token_pattern = (?u)\\\\b\\\\w\\\\w+\\\\b\n",
    "\n",
    "The token_pattern parameter has a default regular expression value of (?u)\\\\b\\\\w\\\\w+\\\\b which ignores all punctuation marks and treats them as delimiters, while accepting alphanumeric strings of length greater than or equal to 2, as individual tokens or words.\n",
    "\n",
    "stop_words\n",
    "\n",
    "The stop_words parameter, if set to english will remove all words from our document set that match a list of English stop words which is defined in scikit-learn. Considering the size of our dataset and the fact that we are dealing with SMS messages and not larger text sources like e-mail, we will not be setting this parameter value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "89698bc13286e251f007438e5143a185874dec8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['are',\n",
       " 'call',\n",
       " 'from',\n",
       " 'hello',\n",
       " 'home',\n",
       " 'how',\n",
       " 'me',\n",
       " 'money',\n",
       " 'now',\n",
       " 'tomorrow',\n",
       " 'win',\n",
       " 'you']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vector.fit(documents)\n",
    "count_vector.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "7bdd5c5b641355b80c84ae24142f5ded3e4054ba"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_array = count_vector.transform(documents).toarray()\n",
    "doc_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "770d9a3f5b134e81664eea69eae48a38bb5a03aa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>are</th>\n",
       "      <th>call</th>\n",
       "      <th>from</th>\n",
       "      <th>hello</th>\n",
       "      <th>home</th>\n",
       "      <th>how</th>\n",
       "      <th>me</th>\n",
       "      <th>money</th>\n",
       "      <th>now</th>\n",
       "      <th>tomorrow</th>\n",
       "      <th>win</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n",
       "0    1     0     0      1     0    1   0      0    0         0    0    1\n",
       "1    0     0     1      0     1    0   0      1    0         0    2    0\n",
       "2    0     1     0      0     0    0   1      0    1         0    0    0\n",
       "3    0     1     0      2     0    0   0      0    0         1    0    1"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())\n",
    "frequency_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before generating document-term matrix, lets separate the SMS data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "1b3e28582c81d0995c2aca898972489cd47f7ffa"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_sms['sms'], \n",
    "                                                    df_sms['label'],test_size=0.20, \n",
    "                                                    random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d84a48a44851dd6b927e8548e8b1cfcc0e4c0470"
   },
   "outputs": [],
   "source": [
    "# Instantiate the CountVectorizer method\n",
    "count_vector = CountVectorizer()\n",
    "\n",
    "# Fit the training data and then return the matrix\n",
    "training_data = count_vector.fit_transform(X_train)\n",
    "\n",
    "# Transform testing data and return the matrix. \n",
    "testing_data = count_vector.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4457, 7777)\n"
     ]
    }
   ],
   "source": [
    "print (training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1115, 7777)\n"
     ]
    }
   ],
   "source": [
    "print (testing_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b765e268c1ab97cc5b9cb0dfd0201ed640397e4c"
   },
   "source": [
    "## Implementation of Naive Bayes Machine Learning Algorithm \n",
    "\n",
    "We use  sklearns **sklearn.naive_bayes** method to make predictions on our dataset.\n",
    "\n",
    "Specifically, we use **multinomial Naive Bayes** implementation. This particular classifier is suitable for classification with discrete features (such as in our case, word counts for text classification). It takes in integer word counts as its input. On the other hand **Gaussian Naive Bayes** is better suited for continuous data as it assumes that the input data has a Gaussian(normal) distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "39a499895458b0d9a6f152dfa9a17e279353d0ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "naive_bayes = MultinomialNB()\n",
    "naive_bayes.fit(training_data,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "6db40f3162c1b627bdb030b4ac6104bf3cc70681"
   },
   "outputs": [],
   "source": [
    "#Make prediction on test dataset\n",
    "predictions = naive_bayes.predict(testing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3edec1b6f6f8dd75c0b8ddaeb37a1007537103e"
   },
   "source": [
    "**Evaluating our model**\n",
    "\n",
    "Now that we have made predictions on our test set, our next goal is to evaluate how well our model is doing. There are various mechanisms for doing so, but first let's do quick recap of them.\n",
    "\n",
    "**Accuracy** measures how often the classifier makes the correct prediction. It’s the ratio of the number of correct predictions to the total number of predictions (the number of test data points).\n",
    "\n",
    "**Precision** tells us what proportion of messages we classified as spam, actually are spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all positives(all words classified as spam, irrespective of whether that was the correct classification), in other words it is the ratio of\n",
    "\n",
    "**[True Positives/(True Positives + False Positives)]**\n",
    "\n",
    "**Recall(sensitivity)** tells us what proportion of messages that actually are spam were classified by us as spam. It is a ratio of true positives(words classified as spam, and which are actually spam) to all the words that were actually spam, in other words it is the ratio of\n",
    "\n",
    "**[True Positives/(True Positives + False Negatives)]**\n",
    "\n",
    "For classification problems that are skewed in their classification distributions like in our case, for example if we had a 100 text messages and only 2 were spam and the rest 98 weren't, accuracy by itself is not a very good metric. We could classify 90 messages as not spam(including the 2 that were spam but we classify them as not spam, hence they would be false negatives) and 10 as spam(all 10 false positives) and still get a reasonably good accuracy score. For such cases, precision and recall come in very handy. These two metrics can be combined to get the F1 score, which is weighted average of the precision and recall scores. This score can range from 0 to 1, with 1 being the best possible F1 score.\n",
    "\n",
    "We will be using all 4 metrics to make sure our model does well. For all 4 metrics whose values can range from 0 to 1, having a score as close to 1 as possible is a good indicator of how well our model is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "0bc4167d3ff888530c292ae0ed7a8e924e8caf30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score: 0.9847533632286996\n",
      "Precision score: 0.9420289855072463\n",
      "Recall score: 0.935251798561151\n",
      "F1 score: 0.9386281588447652\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "print('Accuracy score: {}'.format(accuracy_score(y_test, predictions)))\n",
    "print('Precision score: {}'.format(precision_score(y_test, predictions)))\n",
    "print('Recall score: {}'.format(recall_score(y_test, predictions)))\n",
    "print('F1 score: {}'.format(f1_score(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "db41456bf07a7de127c3368b1f0db87c5632d512"
   },
   "source": [
    "One of the major advantages that **Naive Bayes** has over other classification algorithms is its ability to handle an extremely large number of features. In our case, each word is treated as a feature and there are thousands of different words. Also, it performs well even with the presence of irrelevant features and is relatively unaffected by them.\n",
    "\n",
    "The other major advantage it has is its relative simplicity. Naive Bayes' works well right out of the box and tuning it's parameters is rarely ever necessary, except usually in cases where the distribution of the data is known. \n",
    "\n",
    "It rarely ever overfits the data.\n",
    "\n",
    "Another important advantage is that its model training and prediction times are very fast for the amount of data it can handle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9f9019cfa476a59b9b1233522808d0a3f368faef"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
